{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Model\n",
        "___\n",
        "\n",
        "This notebook demonstrates the complete workflow for training and evaluating a multiclass classification model using Pytorch. The steps include:\n",
        "\n",
        "* to be included"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTnpA365X7r9"
      },
      "outputs": [],
      "source": [
        "# Import standard libraries for randomness, deep copying, and numerical operations\n",
        "import random\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "# Import libraries for image processing and data manipulation\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Import PyTorch core and utilities for deep learning\n",
        "import torch\n",
        "import torch.optim as optim  # Optimization algorithms\n",
        "import torch.nn as nn  # Neural network modules\n",
        "import torch.nn.functional as F  # Functional API for non-parametric operations\n",
        "\n",
        "# Import PyTorch utilities for data loading and transformations\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n",
        "from torchvision.transforms.v2 import Compose, ToImage, Normalize, ToPILImage, Resize, ToDtype\n",
        "\n",
        "# Import dataset handling and learning rate schedulers\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, MultiStepLR, CyclicLR, LambdaLR\n",
        "\n",
        "# Import visualization and web utilities\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import errno\n",
        "\n",
        "# Set matplotlib style for better visuals\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Import custom utilities for architecture, data reading, figures, and metrics\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "from utils.architecture import Architecture\n",
        "from utils.figures import *\n",
        "from utils.images import visualize_images\n",
        "from utils.metrics import get_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9wh98Ryx7Pd"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14goKgXj0CwA"
      },
      "source": [
        "Actual dataset from Mendeley Data:\n",
        "> Amin, Md Al; Mahmud, Md Iqbal; Rahman, Asadullah Bin; Parvin, Mst Aktarina; Mamun, Md Abdulla Al (2024), “Guava Fruit Disease Dataset”, Mendeley Data, V1, doi: 10.17632/bkdkc4n835.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPpGYgTYx7Pe"
      },
      "source": [
        "## ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usfX4Pymx7Pe"
      },
      "outputs": [],
      "source": [
        "# Compose a sequence of preprocessing transforms\n",
        "# 1) Resize images\n",
        "# 2) Ensure output is a PIL/torchvision Image (dropping any alpha channel)\n",
        "# 3) Convert pixel values to float32 and scale from [0–255] to [0.0–1.0]\n",
        "temp_transform = Compose([\n",
        "    Resize(128),                        # Resize each image to 128×128\n",
        "    ToImage(),                         # Convert tensor back to PIL Image (enforces RGB)\n",
        "    ToDtype(torch.float32, scale=True) # Cast to float32 and normalize pixel range\n",
        "])\n",
        "\n",
        "# Create an ImageFolder dataset from the 'train' directory\n",
        "# Images are grouped by subfolder name as class labels, and each image is transformed\n",
        "temp_dataset = ImageFolder(\n",
        "    root='../data/train',  # Path to the training data directory\n",
        "    transform=temp_transform          # Apply the preprocessing pipeline to every image\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKTSUQSSx7Pe",
        "outputId": "c5bfd8e1-9a5a-4bb2-9d94-fafedc9beac6"
      },
      "outputs": [],
      "source": [
        "# the second element of this tuple is the label\n",
        "temp_dataset[0][0].shape, temp_dataset[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOPvDL09AEnV",
        "outputId": "52f94d09-1b6d-48a6-9f26-9eb2ce28c002"
      },
      "outputs": [],
      "source": [
        "temp_dataset[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(temp_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGK-y5KAKymI",
        "outputId": "1bd8589f-4156-49e7-9d32-c6e1290ee9a7"
      },
      "outputs": [],
      "source": [
        "temp_dataset[len(temp_dataset)-1][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye2yR6BLAxys",
        "outputId": "1c2a615f-f55d-42c0-b3ed-1504cdfc55fa"
      },
      "outputs": [],
      "source": [
        "# Get total number of samples in the dataset\n",
        "dataset_size = len(temp_dataset)\n",
        "print(f\"Dataset size: {dataset_size} images\")\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = len(temp_dataset.classes)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Class names: {temp_dataset.classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iterating through each class to plot its images\n",
        "for label in temp_dataset.classes:\n",
        "    \n",
        "    # Specify the path containing the images to visualize\n",
        "    path_to_visualize = f\"../data/train/{label}\"\n",
        "\n",
        "    # Visualize 3 random images\n",
        "    print(label.upper())\n",
        "    visualize_images(path_to_visualize, num_images=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnqFv6gyx7Pe"
      },
      "source": [
        "## Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgMwU7Spx7Pe"
      },
      "outputs": [],
      "source": [
        "temp_loader = DataLoader(temp_dataset, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Eav2U_x7Pe",
        "outputId": "c31062bf-e71b-48bf-bd98-c044309495cc"
      },
      "outputs": [],
      "source": [
        "# Each column represents a channel\n",
        "# first row is the number of data points\n",
        "# second row is the the sum of mean values\n",
        "# third row is the sum of standard deviations\n",
        "first_images, first_labels = next(iter(temp_loader))\n",
        "Architecture.statistics_per_channel(first_images, first_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXu6vXn8x7Pf",
        "outputId": "40dc78b2-4483-4c4b-a88a-1749c1a8634c"
      },
      "outputs": [],
      "source": [
        "# We can leverage the loader_apply() method to get the sums for the whole dataset:\n",
        "results = Architecture.loader_apply(temp_loader, Architecture.statistics_per_channel)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWfYfTQZx7Pf",
        "outputId": "8d243385-a464-4abd-9228-b7840106be5f"
      },
      "outputs": [],
      "source": [
        "# we can compute the average mean value and the average standard deviation, per channel.\n",
        "# Better yet, let’s make it a method that takes a data loader and\n",
        "# returns an instance of the Normalize() transform\n",
        "normalizer = Architecture.make_normalizer(temp_loader)\n",
        "normalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVk-FutYwt00"
      },
      "source": [
        "> Remember that PyTorch converts the pixel values into the [0, 1] range. The average mean value of a pixel for the red (first) channel is 0.8502, while its average standard deviation is 0.2089."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxBYRaX4xOfl"
      },
      "source": [
        ">> **IMPORTANT**: Always use the training set to compute statistics\n",
        "for standardization! This avoid data leakage!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OewfD2eLx7Pf"
      },
      "source": [
        "## The Real Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy5Gz8sgx7Pf"
      },
      "outputs": [],
      "source": [
        "# Define a pipeline of image transformations:\n",
        "# 1) Resize each image to 28×28 pixels\n",
        "# 2) Ensure the output is a PIL/torchvision image (dropping any alpha channel)\n",
        "# 3) Cast pixels to float32 and scale from [0–255] to [0.0–1.0]\n",
        "# 4) Apply the user-defined normalization (e.g., mean/std normalization)\n",
        "composer = Compose([\n",
        "    Resize(128),                         # Resize to 28×28\n",
        "    ToImage(),                          # Convert to PIL Image in RGB\n",
        "    ToDtype(torch.float32, scale=True), # Cast to float32 and normalize to [0,1]\n",
        "    # normalizer                          # Apply custom normalization transform\n",
        "])\n",
        "\n",
        "# Instantiate training and validation datasets from folders:\n",
        "# - 'rps' contains subfolders per class for training\n",
        "# - 'rps-test-set' likewise for validation\n",
        "train_data = ImageFolder(root='../data/train', transform=composer)\n",
        "val_data   = ImageFolder(root='../data/val', transform=composer)\n",
        "test_data   = ImageFolder(root='../data/test', transform=composer)\n",
        "\n",
        "# Wrap datasets in DataLoaders for batching and shuffling:\n",
        "# - batch_size=16 yields mini-batches of 16 images\n",
        "# - shuffle=True randomizes training order each epoch\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_data,   batch_size=128)  # no shuffle for validation\n",
        "test_loader   = DataLoader(val_data,   batch_size=128)  # no shuffle for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total = len(train_data) + len(val_data) + len(test_data)\n",
        "train_pct = len(train_data) / total * 100\n",
        "val_pct = len(val_data) / total * 100\n",
        "test_pct = len(test_data) / total * 100\n",
        "\n",
        "print(f\"Treinamento: {train_pct:.2f}%\")\n",
        "print(f\"Validação: {val_pct:.2f}%\")\n",
        "print(f\"Teste: {test_pct:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMKSojnWycbe"
      },
      "outputs": [],
      "source": [
        "def figure2(first_images, first_labels):\n",
        "    fig, axs = plt.subplots(1, 6, figsize=(12, 4))\n",
        "    titles = ['Anthracnose', 'fruit_fly', 'healthy_guava']\n",
        "    for i in range(6):\n",
        "        image, label = ToPILImage()(first_images[i]), first_labels[i]\n",
        "        axs[i].imshow(image)\n",
        "        axs[i].set_xticks([])\n",
        "        axs[i].set_yticks([])\n",
        "        axs[i].set_title(titles[label], fontsize=12)\n",
        "    fig.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "W9RgYxnAx7Pf",
        "outputId": "fa093508-2d5c-4bbc-d360-4a853170f3bd"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(88)\n",
        "first_images, first_labels = next(iter(train_loader))\n",
        "\n",
        "fig = figure2(first_images, first_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl0_Fj1mzXSr"
      },
      "source": [
        "There is nothing wrong with the colors, it is just the effect of the standardization\n",
        "of the pixel values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp9hamqkx7Pg"
      },
      "source": [
        "# Model configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj9Az9zIgs8l"
      },
      "source": [
        "Let’s leave the Sequential model aside for now and build a model class again. This\n",
        "time, our constructor method will take two arguments: ``n_filters`` and ``p``.\n",
        "\n",
        "We’ll use\n",
        "``n_filters`` as the number of output channels for both convolutional blocks of our\n",
        "model (yes, there are two now!). And, as you can see from the code below, we’ll use\n",
        "``p`` as the probability of **dropout**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8w4MPnpx7Pg"
      },
      "outputs": [],
      "source": [
        "class CNN2(nn.Module):\n",
        "    def __init__(self, n_feature, p=0.0):\n",
        "        super(CNN2, self).__init__()\n",
        "        self.n_feature = n_feature\n",
        "        self.p = p\n",
        "        # Creates the convolution layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3,\n",
        "                               out_channels=n_feature,\n",
        "                               kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=n_feature,\n",
        "                               out_channels=n_feature,\n",
        "                               kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(in_channels=n_feature,\n",
        "                               out_channels=n_feature,\n",
        "                               kernel_size=3)\n",
        "        # Creates the linear layers\n",
        "        # Where do this 5 * 5 come from?! Check it below\n",
        "        # self.fc1 = nn.Linear(n_feature * 5 * 5, 50)\n",
        "        # self.fc2 = nn.Linear(50, 3)\n",
        "        self.fc1 = nn.Linear(27, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 3)\n",
        "        # Creates dropout layers\n",
        "        self.drop = nn.Dropout(self.p)\n",
        "\n",
        "    def featurizer(self, x):\n",
        "        # Featurizer\n",
        "        # First convolutional block\n",
        "        # 3@28x28 -> n_feature@26x26 -> n_feature@13x13\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=3)\n",
        "        # Second convolutional block\n",
        "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=3)\n",
        "        # Input dimension (n_feature@5x5)\n",
        "        # Output dimension (n_feature * 5 * 5)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=3)\n",
        "        # Input dimension (n_feature@5x5)\n",
        "        # Output dimension (n_feature * 5 * 5)\n",
        "        x = nn.Flatten()(x)\n",
        "        return x\n",
        "\n",
        "    def classifier(self, x):\n",
        "        # Classifier\n",
        "        # Hidden Layer\n",
        "        # Input dimension (n_feature * 5 * 5)\n",
        "        # Output dimension (50)\n",
        "        if self.p > 0:\n",
        "            x = self.drop(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        # Output Layer\n",
        "        # Input dimension (50)\n",
        "        # Output dimension (3)\n",
        "        if self.p > 0:\n",
        "            x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        if self.p > 0:\n",
        "            x = self.drop(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.featurizer(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU9Sv3_rW_Uz"
      },
      "source": [
        "# Case Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjwBhjA1x7Pl"
      },
      "source": [
        "## Model Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8AnsR9RGCaN"
      },
      "source": [
        "The configuration part is short and straightforward:\n",
        "- We create a model,\n",
        "-  a loss function,\n",
        "- and an optimizer.\n",
        "\n",
        "The model will be an instance of our CNN2 class with five filters and a dropout\n",
        "probability of 30%.\n",
        "\n",
        "Our dataset has three classes, so we’re using\n",
        "``nn.CrossEntropyLoss()`` (which will take the three logits produced by our model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkrMKNDex7Pl"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "\n",
        "# Model/Architecture\n",
        "model_cnn2 = CNN2(n_feature=3, p=0.3)\n",
        "\n",
        "# Loss function\n",
        "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "# Optimizer\n",
        "optimizer_cnn2 = optim.Adam(model_cnn2.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h67IlmE6Gg1E"
      },
      "source": [
        "> Adaptive moment estimation (Adam) uses adaptive learning\n",
        "rates, computing a learning rate for each parameter. Yes, you\n",
        "read it right: Each parameter has a learning rate to call its own!\n",
        "\n",
        "\n",
        "> If you dig into the state_dict() of an Adam optimizer, you’ll find\n",
        "tensors shaped like the parameters of every layer in your model\n",
        "that Adam will use to compute the corresponding learning rates.\n",
        "True story!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCcNFhatLlF7",
        "outputId": "743c4e31-8eb6-4c26-f15d-d5ffb112b39d"
      },
      "outputs": [],
      "source": [
        "optimizer_cnn2.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arch_cnn2 = Architecture(model_cnn2,\n",
        "                        multi_loss_fn,\n",
        "                        optimizer_cnn2)\n",
        "arch_cnn2.set_loaders(train_loader, val_loader)\n",
        "arch_cnn2.train(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fiding LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_lr_fn(start_lr, end_lr, num_iter, step_mode='exp'):\n",
        "    if step_mode == 'linear':\n",
        "        factor = (end_lr / start_lr - 1) / num_iter\n",
        "        def lr_fn(iteration):\n",
        "            return 1 + iteration * factor\n",
        "    else:\n",
        "        factor = (np.log(end_lr) - np.log(start_lr)) / num_iter    \n",
        "        def lr_fn(iteration):\n",
        "            return np.exp(factor)**iteration    \n",
        "    return lr_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_lr = 0.01\n",
        "end_lr = 0.1\n",
        "num_iter = 10\n",
        "lr_fn = make_lr_fn(start_lr, end_lr, num_iter, step_mode='exp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_fn(np.arange(num_iter + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_lr * lr_fn(np.arange(num_iter + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dummy_optimizer = optim.Adam(model_cnn2.parameters(), lr=start_lr)\n",
        "dummy_scheduler = LambdaLR(dummy_optimizer, lr_lambda=lr_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dummy_optimizer.step()\n",
        "dummy_scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dummy_scheduler.get_last_lr()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lr_range_test(self, data_loader, end_lr, num_iter=100, step_mode='exp', alpha=0.05, ax=None):\n",
        "    # Since the test updates both model and optimizer we need to store\n",
        "    # their initial states to restore them in the end\n",
        "    previous_states = {'model': deepcopy(self.model.state_dict()), \n",
        "                       'optimizer': deepcopy(self.optimizer.state_dict())}\n",
        "    # Retrieves the learning rate set in the optimizer\n",
        "    start_lr = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
        "    \n",
        "    # Builds a custom function and corresponding scheduler\n",
        "    lr_fn = make_lr_fn(start_lr, end_lr, num_iter)\n",
        "    scheduler = LambdaLR(self.optimizer, lr_lambda=lr_fn)\n",
        "\n",
        "    # Variables for tracking results and iterations\n",
        "    tracking = {'loss': [], 'lr': []}\n",
        "    iteration = 0\n",
        "\n",
        "    # If there are more iterations than mini-batches in the data loader,\n",
        "    # it will have to loop over it more than once\n",
        "    while (iteration < num_iter):\n",
        "        # That's the typical mini-batch inner loop\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            y_batch = y_batch.to(self.device)\n",
        "            # Step 1\n",
        "            yhat = self.model(x_batch)\n",
        "            # Step 2\n",
        "            loss = self.loss_fn(yhat, y_batch)\n",
        "            # Step 3\n",
        "            loss.backward()\n",
        "\n",
        "            # Here we keep track of the losses (smoothed)\n",
        "            # and the learning rates\n",
        "            tracking['lr'].append(scheduler.get_last_lr()[0])\n",
        "            if iteration == 0:\n",
        "                tracking['loss'].append(loss.item())\n",
        "            else:\n",
        "                prev_loss = tracking['loss'][-1]\n",
        "                smoothed_loss = alpha * loss.item() + (1-alpha) * prev_loss\n",
        "                tracking['loss'].append(smoothed_loss)\n",
        "\n",
        "            iteration += 1\n",
        "            # Number of iterations reached\n",
        "            if iteration == num_iter:\n",
        "                break\n",
        "\n",
        "            # Step 4\n",
        "            self.optimizer.step()\n",
        "            scheduler.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "    # Restores the original states\n",
        "    self.optimizer.load_state_dict(previous_states['optimizer'])\n",
        "    self.model.load_state_dict(previous_states['model'])\n",
        "    \n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
        "    else:\n",
        "        fig = ax.get_figure()\n",
        "    ax.plot(tracking['lr'], tracking['loss'])\n",
        "    if step_mode == 'exp':\n",
        "        ax.set_xscale('log')\n",
        "    ax.set_xlabel('Learning Rate')\n",
        "    ax.set_ylabel('Loss')\n",
        "    fig.tight_layout()\n",
        "    return tracking, fig\n",
        "\n",
        "setattr(arch_cnn2, 'lr_range_test', lr_range_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "new_model = CNN2(n_feature=3, p=0.3)\n",
        "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "new_optimizer = optim.Adam(new_model.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sbs_new = arch_cnn2(new_model, multi_loss_fn, new_optimizer)\n",
        "tracking, fig = sbs_new.lr_range_test(train_loader, end_lr=1e-1, num_iter=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_optimizer(self, optimizer):\n",
        "    self.optimizer = optimizer\n",
        "    \n",
        "setattr(arch_cnn2, 'set_optimizer', set_optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_optimizer = optim.Adam(new_model.parameters(), lr=0.005)\n",
        "sbs_new.set_optimizer(new_optimizer)\n",
        "sbs_new.set_loaders(train_loader, val_loader)\n",
        "sbs_new.train(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "fruit-disease-classification",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
